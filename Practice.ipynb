{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e703b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33331734 0.3333183  0.33336434]\n",
      " [0.3332888  0.33329153 0.33341965]\n",
      " [0.33325943 0.33326396 0.33347666]\n",
      " [0.33323312 0.33323926 0.33352762]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "class dense_layer:\n",
    "    def __init__(self,n_input,neuron):\n",
    "        self.weight = 0.1*np.random.randn(n_input,neuron)\n",
    "        self.biases = np.zeros((1,neuron))\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output = np.dot(input,self.weight)+self.biases\n",
    "        return output\n",
    "    \n",
    "class Relu:\n",
    "    def forward(self,input):\n",
    "            return np.maximum(0,input)\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self,input):\n",
    "        exp = np.exp(input - np.max(input, axis=1,keepdims=True))\n",
    "        output = exp/np.sum(exp,axis=1,keepdims=True) \n",
    "        return output\n",
    "\n",
    "x,y = spiral_data(classes=3,samples=100)\n",
    "\n",
    "first_layer = dense_layer(2,3)\n",
    "output1 = first_layer.forward(x)\n",
    "\n",
    "activation = Relu()\n",
    "activation_result = activation.forward(output1)\n",
    "\n",
    "second_layer = dense_layer(3,3)\n",
    "output2 = second_layer.forward(activation_result)\n",
    "\n",
    "SM = Softmax()\n",
    "final_result = SM.forward(output2)\n",
    "\n",
    "print(final_result[:5])\n",
    "len(final_result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CROSS_ENTROPY:\n",
    "    def forward(self,target,input):\n",
    "        samples = len(input)\n",
    "        clipping = np.clip(input,1e-7,1 - 1e-7)\n",
    "        if len(input.shape) == 1:\n",
    "            confidence = clipping[range(samples),target]\n",
    "            \n",
    "        else:\n",
    "            confidence_matrix = clipping * target\n",
    "            confidence = np.sum(confidence_matrix,keepdims=True,axis=1)\n",
    "        \n",
    "        loss = -np.log(confidence)\n",
    "        \n",
    "        return loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7161d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class dense_layer:\n",
    "    def __init__(self,n_input,neurons):\n",
    "        self.weights = 0.01*np.random.randn(n_input,neurons)\n",
    "        self.bias = np.zeros((1,neurons))\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(self.inputs,self.weights)+self.bias\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "       \n",
    "class Relu :\n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        self.output = np.maximum(0,input)\n",
    "        \n",
    "    def backward(self,dvalue):\n",
    "        self.dinput = dvalue.copy()\n",
    "        self.dinput[self.input<=0] = 0\n",
    "        \n",
    "        \n",
    "class Softmax:\n",
    "    def forward(self,input):\n",
    "        exp = np.exp(input - np.max(input,axis=1,keepdims=True))\n",
    "        prob = exp/np.sum(exp,axis = 1,keepdims=True)\n",
    "        return prob\n",
    "    \n",
    "class Loss:\n",
    "    def calculate(self,y_pred,y_true):\n",
    "        loss = self.forward(y_pred,y_true)\n",
    "        total_loss = np.mean(loss)\n",
    "        return total_loss\n",
    "\n",
    "class Categorical_CE(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        \n",
    "        y_clip = np.clip(y_pred,1e-7,1-1e-7)\n",
    "        if len(y_true.shape) == 2:\n",
    "            correct_confidence = np.sum(y_clip * y_true,axis = 1)\n",
    "            \n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_clip[range(len(y_pred)),y_true ]\n",
    "            \n",
    "        negative_likhelihood = -np.log(correct_confidence)\n",
    "        \n",
    "        return negative_likhelihood\n",
    "    \n",
    "class Softmax_and_Loss_activation:\n",
    "    def __init__(self):\n",
    "        self.softmax = Softmax()\n",
    "        self.loss = Categorical_CE()\n",
    "        \n",
    "    def forward(self,input,y):\n",
    "        self.y_true = y\n",
    "        activation = self.softmax.forward(input)\n",
    "        self.output = activation\n",
    "        return self.loss.calculate(self.output,self.y_true)\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        samples = len(dvalues)\n",
    "        if len(self.y_true.shape)==2:\n",
    "            self.y_true = np.argmax(self.y_true,axis = 1)\n",
    "        self.dinput = dvalues.copy()\n",
    "        self.dinput[range(samples),self.y_true]-=1\n",
    "        self.dinput = self.dinput/samples\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    def __init__(self,learning_rate=1.,decay=0.,momentum = 0,epsilon=1e-7):\n",
    "        self.lr = learning_rate\n",
    "        self.current_learning = learning_rate\n",
    "        self.decay = decay\n",
    "        self.itteration = 0\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def preupdate(self):\n",
    "        if self.decay:\n",
    "            self.current_learning = self.lr / (1 + (self.decay * self.itteration))\n",
    "    def update(self,layer):\n",
    "        if self.momentum :\n",
    "\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.bias)\n",
    "                \n",
    "            weight_update = -(self.current_learning *layer.dweights )+(layer.weight_momentums * self.momentum) #direction\n",
    "            layer.weight_momentums = weight_update \n",
    "            bias_update = -(self.current_learning *layer.dbiases )+(layer.bias_momentums * self.momentum)\n",
    "            layer.bias_momentums = bias_update  \n",
    "            \n",
    "        else:    \n",
    "            weight_update = -self.current_learning *layer.dweights\n",
    "            bias_update   = -self.current_learning * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_update\n",
    "        layer.bias += bias_update\n",
    "    \n",
    "    def postupdate(self):\n",
    "        self.itteration+=1\n",
    "\n",
    "        \n",
    "class Optimizer_ADAGRAD:\n",
    "    def __init__(self,layer,epsilon=1e-7,decay=0.,lr=1):\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.current_lr = lr\n",
    "        self.itteration = 0\n",
    "    def preupdate(self):\n",
    "        self.current_lr = self.lr / (1 + (self.decay * self.itteration))\n",
    "    \n",
    "    def update(self,layer):\n",
    "        if not hasattr(layer,'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.bias)\n",
    "                \n",
    "            layer.weight_cache += layer.dweights**2\n",
    "            layer.bias_cache += layer.dbiases**2\n",
    "            \n",
    "            layer.weights += -self.current_learning * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "            layer.bias += -self.current_learning * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)    \n",
    "        \n",
    "        \n",
    "    def postupdate(self):\n",
    "        self.itteration+=1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c42ee3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.333 loss : 1.099\n",
      "epoch: 100, acc: 0.418 loss : 1.083\n",
      "epoch: 200, acc: 0.418 loss : 1.072\n",
      "epoch: 300, acc: 0.423 loss : 1.071\n",
      "epoch: 400, acc: 0.421 loss : 1.071\n",
      "epoch: 500, acc: 0.419 loss : 1.070\n",
      "epoch: 600, acc: 0.417 loss : 1.070\n",
      "epoch: 700, acc: 0.420 loss : 1.070\n",
      "epoch: 800, acc: 0.420 loss : 1.070\n",
      "epoch: 900, acc: 0.422 loss : 1.069\n",
      "epoch: 1000, acc: 0.427 loss : 1.068\n",
      "epoch: 1100, acc: 0.428 loss : 1.067\n",
      "epoch: 1200, acc: 0.429 loss : 1.066\n",
      "epoch: 1300, acc: 0.432 loss : 1.064\n",
      "epoch: 1400, acc: 0.432 loss : 1.061\n",
      "epoch: 1500, acc: 0.433 loss : 1.058\n",
      "epoch: 1600, acc: 0.436 loss : 1.055\n",
      "epoch: 1700, acc: 0.436 loss : 1.050\n",
      "epoch: 1800, acc: 0.438 loss : 1.046\n",
      "epoch: 1900, acc: 0.434 loss : 1.040\n",
      "epoch: 2000, acc: 0.453 loss : 1.035\n",
      "epoch: 2100, acc: 0.470 loss : 1.029\n",
      "epoch: 2200, acc: 0.467 loss : 1.023\n",
      "epoch: 2300, acc: 0.480 loss : 1.016\n",
      "epoch: 2400, acc: 0.474 loss : 1.010\n",
      "epoch: 2500, acc: 0.459 loss : 1.003\n",
      "epoch: 2600, acc: 0.462 loss : 0.997\n",
      "epoch: 2700, acc: 0.468 loss : 0.992\n",
      "epoch: 2800, acc: 0.477 loss : 0.986\n",
      "epoch: 2900, acc: 0.483 loss : 0.981\n",
      "epoch: 3000, acc: 0.492 loss : 0.974\n",
      "epoch: 3100, acc: 0.498 loss : 0.967\n",
      "epoch: 3200, acc: 0.506 loss : 0.960\n",
      "epoch: 3300, acc: 0.514 loss : 0.954\n",
      "epoch: 3400, acc: 0.521 loss : 0.948\n",
      "epoch: 3500, acc: 0.522 loss : 0.942\n",
      "epoch: 3600, acc: 0.520 loss : 0.937\n",
      "epoch: 3700, acc: 0.524 loss : 0.932\n",
      "epoch: 3800, acc: 0.533 loss : 0.927\n",
      "epoch: 3900, acc: 0.541 loss : 0.922\n",
      "epoch: 4000, acc: 0.550 loss : 0.917\n",
      "epoch: 4100, acc: 0.560 loss : 0.912\n",
      "epoch: 4200, acc: 0.569 loss : 0.907\n",
      "epoch: 4300, acc: 0.574 loss : 0.902\n",
      "epoch: 4400, acc: 0.582 loss : 0.897\n",
      "epoch: 4500, acc: 0.592 loss : 0.892\n",
      "epoch: 4600, acc: 0.602 loss : 0.887\n",
      "epoch: 4700, acc: 0.600 loss : 0.883\n",
      "epoch: 4800, acc: 0.599 loss : 0.878\n",
      "epoch: 4900, acc: 0.599 loss : 0.873\n",
      "epoch: 5000, acc: 0.601 loss : 0.869\n",
      "epoch: 5100, acc: 0.599 loss : 0.864\n",
      "epoch: 5200, acc: 0.590 loss : 0.860\n",
      "epoch: 5300, acc: 0.593 loss : 0.855\n",
      "epoch: 5400, acc: 0.598 loss : 0.851\n",
      "epoch: 5500, acc: 0.602 loss : 0.847\n",
      "epoch: 5600, acc: 0.609 loss : 0.842\n",
      "epoch: 5700, acc: 0.616 loss : 0.838\n",
      "epoch: 5800, acc: 0.619 loss : 0.834\n",
      "epoch: 5900, acc: 0.620 loss : 0.830\n",
      "epoch: 6000, acc: 0.620 loss : 0.826\n",
      "epoch: 6100, acc: 0.622 loss : 0.823\n",
      "epoch: 6200, acc: 0.621 loss : 0.819\n",
      "epoch: 6300, acc: 0.623 loss : 0.815\n",
      "epoch: 6400, acc: 0.627 loss : 0.812\n",
      "epoch: 6500, acc: 0.630 loss : 0.808\n",
      "epoch: 6600, acc: 0.633 loss : 0.805\n",
      "epoch: 6700, acc: 0.636 loss : 0.802\n",
      "epoch: 6800, acc: 0.631 loss : 0.798\n",
      "epoch: 6900, acc: 0.630 loss : 0.795\n",
      "epoch: 7000, acc: 0.630 loss : 0.792\n",
      "epoch: 7100, acc: 0.629 loss : 0.789\n",
      "epoch: 7200, acc: 0.629 loss : 0.786\n",
      "epoch: 7300, acc: 0.631 loss : 0.783\n",
      "epoch: 7400, acc: 0.628 loss : 0.781\n",
      "epoch: 7500, acc: 0.628 loss : 0.778\n",
      "epoch: 7600, acc: 0.627 loss : 0.776\n",
      "epoch: 7700, acc: 0.627 loss : 0.773\n",
      "epoch: 7800, acc: 0.627 loss : 0.770\n",
      "epoch: 7900, acc: 0.627 loss : 0.768\n",
      "epoch: 8000, acc: 0.629 loss : 0.766\n",
      "epoch: 8100, acc: 0.627 loss : 0.763\n",
      "epoch: 8200, acc: 0.623 loss : 0.761\n",
      "epoch: 8300, acc: 0.627 loss : 0.759\n",
      "epoch: 8400, acc: 0.630 loss : 0.757\n",
      "epoch: 8500, acc: 0.631 loss : 0.754\n",
      "epoch: 8600, acc: 0.633 loss : 0.753\n",
      "epoch: 8700, acc: 0.637 loss : 0.751\n",
      "epoch: 8800, acc: 0.636 loss : 0.749\n",
      "epoch: 8900, acc: 0.637 loss : 0.748\n",
      "epoch: 9000, acc: 0.637 loss : 0.746\n",
      "epoch: 9100, acc: 0.640 loss : 0.744\n",
      "epoch: 9200, acc: 0.640 loss : 0.742\n",
      "epoch: 9300, acc: 0.641 loss : 0.740\n",
      "epoch: 9400, acc: 0.641 loss : 0.736\n",
      "epoch: 9500, acc: 0.641 loss : 0.733\n",
      "epoch: 9600, acc: 0.654 loss : 0.730\n",
      "epoch: 9700, acc: 0.660 loss : 0.727\n",
      "epoch: 9800, acc: 0.664 loss : 0.724\n",
      "epoch: 9900, acc: 0.662 loss : 0.722\n",
      "epoch: 10000, acc: 0.668 loss : 0.719\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "x,y = spiral_data(classes=3,samples=300)\n",
    "\n",
    "layer1 = dense_layer(2,64)\n",
    "activation1 = Relu()\n",
    "layer2 = dense_layer(64,3)\n",
    "Softmaxandloss = Softmax_and_Loss_activation()\n",
    "optimizer = Optimizer_SGD(decay=1e-3)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    layer1.forward(x)\n",
    "\n",
    "    activation1.forward(layer1.output)\n",
    "\n",
    "    layer2.forward(activation1.output)\n",
    "\n",
    "    loss = Softmaxandloss.forward(layer2.output,y)\n",
    "\n",
    "    prediction = np.argmax(Softmaxandloss.output,axis=1)\n",
    "    if len(y.shape)==2:\n",
    "        y = np.argmax(y,axis = 1)\n",
    "    accuracy = np.mean(prediction==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, '+\n",
    "            f'acc: {accuracy:.3f} '+\n",
    "            f'loss : {loss:.3f}')\n",
    "\n",
    "\n",
    "    Softmaxandloss.backward(Softmaxandloss.output)\n",
    "    layer2.backward(Softmaxandloss.dinput)\n",
    "    activation1.backward(layer2.dinputs)\n",
    "    layer1.backward(activation1.dinput)\n",
    "    \n",
    "    optimizer.preupdate()\n",
    "    optimizer.update(layer1)\n",
    "    optimizer.update(layer2)\n",
    "    optimizer.postupdate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbff60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Dense_layer:\n",
    "    def __init__(self,n_input,neuron):\n",
    "        self.weights = 0.01*np.random.randn(n_input,neuron)\n",
    "        self.biases = np.zeros((1,neuron))\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(input,self.weights)+self.biases\n",
    "    def backward(self,dvalue):\n",
    "        self.dweights = np.dot(self.input.T,dvalue)\n",
    "        self.dbiases = np.sum(dvalue, axis=0, keepdims=True)\n",
    "        self.dinput = np.dot(dvalue,self.weights.T)\n",
    "    \n",
    "class Relu:\n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        self.output = np.maximum(0,self.input)\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.input <= 0] = 0\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self,input):\n",
    "        exp = np.exp(input - np.max(input,axis=1,keepdims=True))\n",
    "        prob = exp/np.sum(exp,axis=1,keepdims=True)\n",
    "        return prob\n",
    "        \n",
    "class Loss:\n",
    "    def calculate(self,y_pred,y_true):\n",
    "        loss = self.forward(y_pred,y_true)\n",
    "        total_loss = np.mean(loss)\n",
    "        return total_loss\n",
    "    \n",
    "    \n",
    "class CrossCategorical(Loss) :\n",
    "    def forward(self,y_pred,y_true):\n",
    "        y_pred_clip = np.clip(y_pred,1e-7,1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 2:\n",
    "            correct_confidence = np.sum(y_true * y_pred_clip,axis=1)\n",
    "        \n",
    "        elif len(y_true.shape) == 1:\n",
    "            correct_confidence = y_pred_clip[range(len(y_pred)),y_true]\n",
    "            \n",
    "        negative_likelihood = -np.log(correct_confidence)\n",
    "        \n",
    "        return negative_likelihood\n",
    "    \n",
    "class Softmax_and_Loss_CE:\n",
    "    def __init__(self):\n",
    "        self.softmax = Softmax()\n",
    "        self.loss = CrossCategorical()\n",
    "        \n",
    "    def forward(self,input,y_true):\n",
    "        self.output = self.softmax.forward(input)\n",
    "        return self.loss.calculate(self.output,y_true)\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        self.dinput = dvalues.copy()\n",
    "        if len(y_true.shape)==2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        self.dinput[range(len(dvalues)),y_true] -= 1\n",
    "        self.dinput = self.dinput/len(dvalues)\n",
    "        \n",
    "class Adam_Optimizer:\n",
    "    def __init__(self,epsilon= 1e-7,lr = 1,decay = 0, beta1 = 0.9,beta2=0.999):\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.current_lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.itteration = 0\n",
    "        self.decay = decay\n",
    "        \n",
    "    def preupdate(self):\n",
    "        if self.decay:\n",
    "            self.current_lr = self.lr / (1 + (self.decay * self.itteration)) \n",
    "    \n",
    "    def update(self,layer):\n",
    "        if not hasattr(layer,'weights_cache'):\n",
    "            layer.weights_momentum = np.zeros_like(layer.weights)\n",
    "            layer.biases_momentum = np.zeros_like(layer.biases)\n",
    "            layer.weights_cache = np.zeros_like(layer.weights)\n",
    "            layer.biases_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        layer.weights_momentum = (self.beta1 * layer.weights_momentum) + ((1-self.beta1) * layer.dweights)\n",
    "        corrected_weights_momentum = layer.weights_momentum / (1-self.beta1**(self.itteration+1))\n",
    "        layer.biases_momentum =  (self.beta1 * layer.biases_momentum) + ((1-self.beta1) * layer.dbiases)\n",
    "        corrected_bias_momentum = layer.biases_momentum / (1-self.beta1**(self.itteration+1))\n",
    "        \n",
    "        layer.weights_cache = (self.beta2 * layer.weights_cache) + ((1-self.beta2) * (layer.dweights**2))\n",
    "        corrected_weights_cache = layer.weights_cache / (1-self.beta2**(self.itteration+1))\n",
    "        layer.biases_cache = (self.beta2 * layer.biases_cache) + ((1-self.beta2) * (layer.dbiases**2))\n",
    "        corrected_bias_cache = layer.biases_cache / (1-self.beta2**(self.itteration+1))\n",
    "        \n",
    "        layer.weights += -(self.current_lr * corrected_weights_momentum/(np.sqrt(corrected_weights_cache)+self.epsilon))\n",
    "        layer.biases += -(self.current_lr * corrected_bias_momentum/(np.sqrt(corrected_bias_cache)+self.epsilon))\n",
    "    \n",
    "    def postupdate(self):\n",
    "        self.itteration += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67352b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0.000 Accuracy :0.360 Loss : 1.099\n",
      "epoch : 100.000 Accuracy :0.463 Loss : 0.995\n",
      "epoch : 200.000 Accuracy :0.639 Loss : 0.863\n",
      "epoch : 300.000 Accuracy :0.704 Loss : 0.761\n",
      "epoch : 400.000 Accuracy :0.717 Loss : 0.702\n",
      "epoch : 500.000 Accuracy :0.732 Loss : 0.658\n",
      "epoch : 600.000 Accuracy :0.751 Loss : 0.608\n",
      "epoch : 700.000 Accuracy :0.769 Loss : 0.574\n",
      "epoch : 800.000 Accuracy :0.777 Loss : 0.545\n",
      "epoch : 900.000 Accuracy :0.780 Loss : 0.515\n",
      "epoch : 1000.000 Accuracy :0.794 Loss : 0.489\n",
      "epoch : 1100.000 Accuracy :0.806 Loss : 0.466\n",
      "epoch : 1200.000 Accuracy :0.820 Loss : 0.447\n",
      "epoch : 1300.000 Accuracy :0.828 Loss : 0.431\n",
      "epoch : 1400.000 Accuracy :0.836 Loss : 0.416\n",
      "epoch : 1500.000 Accuracy :0.839 Loss : 0.402\n",
      "epoch : 1600.000 Accuracy :0.838 Loss : 0.389\n",
      "epoch : 1700.000 Accuracy :0.847 Loss : 0.379\n",
      "epoch : 1800.000 Accuracy :0.849 Loss : 0.369\n",
      "epoch : 1900.000 Accuracy :0.858 Loss : 0.361\n",
      "epoch : 2000.000 Accuracy :0.863 Loss : 0.353\n",
      "epoch : 2100.000 Accuracy :0.862 Loss : 0.346\n",
      "epoch : 2200.000 Accuracy :0.866 Loss : 0.339\n",
      "epoch : 2300.000 Accuracy :0.867 Loss : 0.334\n",
      "epoch : 2400.000 Accuracy :0.867 Loss : 0.328\n",
      "epoch : 2500.000 Accuracy :0.872 Loss : 0.323\n",
      "epoch : 2600.000 Accuracy :0.873 Loss : 0.318\n",
      "epoch : 2700.000 Accuracy :0.874 Loss : 0.314\n",
      "epoch : 2800.000 Accuracy :0.878 Loss : 0.310\n",
      "epoch : 2900.000 Accuracy :0.876 Loss : 0.307\n",
      "epoch : 3000.000 Accuracy :0.879 Loss : 0.304\n",
      "epoch : 3100.000 Accuracy :0.880 Loss : 0.299\n",
      "epoch : 3200.000 Accuracy :0.880 Loss : 0.294\n",
      "epoch : 3300.000 Accuracy :0.883 Loss : 0.291\n",
      "epoch : 3400.000 Accuracy :0.884 Loss : 0.287\n",
      "epoch : 3500.000 Accuracy :0.887 Loss : 0.285\n",
      "epoch : 3600.000 Accuracy :0.884 Loss : 0.283\n",
      "epoch : 3700.000 Accuracy :0.886 Loss : 0.280\n",
      "epoch : 3800.000 Accuracy :0.889 Loss : 0.278\n",
      "epoch : 3900.000 Accuracy :0.890 Loss : 0.277\n",
      "epoch : 4000.000 Accuracy :0.891 Loss : 0.275\n",
      "epoch : 4100.000 Accuracy :0.892 Loss : 0.273\n",
      "epoch : 4200.000 Accuracy :0.892 Loss : 0.271\n",
      "epoch : 4300.000 Accuracy :0.893 Loss : 0.270\n",
      "epoch : 4400.000 Accuracy :0.894 Loss : 0.268\n",
      "epoch : 4500.000 Accuracy :0.894 Loss : 0.267\n",
      "epoch : 4600.000 Accuracy :0.896 Loss : 0.266\n",
      "epoch : 4700.000 Accuracy :0.894 Loss : 0.265\n",
      "epoch : 4800.000 Accuracy :0.896 Loss : 0.263\n",
      "epoch : 4900.000 Accuracy :0.897 Loss : 0.262\n",
      "epoch : 5000.000 Accuracy :0.896 Loss : 0.261\n",
      "epoch : 5100.000 Accuracy :0.892 Loss : 0.260\n",
      "epoch : 5200.000 Accuracy :0.897 Loss : 0.259\n",
      "epoch : 5300.000 Accuracy :0.898 Loss : 0.258\n",
      "epoch : 5400.000 Accuracy :0.898 Loss : 0.257\n",
      "epoch : 5500.000 Accuracy :0.896 Loss : 0.256\n",
      "epoch : 5600.000 Accuracy :0.897 Loss : 0.256\n",
      "epoch : 5700.000 Accuracy :0.899 Loss : 0.255\n",
      "epoch : 5800.000 Accuracy :0.896 Loss : 0.254\n",
      "epoch : 5900.000 Accuracy :0.897 Loss : 0.253\n",
      "epoch : 6000.000 Accuracy :0.900 Loss : 0.252\n",
      "epoch : 6100.000 Accuracy :0.901 Loss : 0.252\n",
      "epoch : 6200.000 Accuracy :0.899 Loss : 0.251\n",
      "epoch : 6300.000 Accuracy :0.900 Loss : 0.250\n",
      "epoch : 6400.000 Accuracy :0.900 Loss : 0.249\n",
      "epoch : 6500.000 Accuracy :0.899 Loss : 0.249\n",
      "epoch : 6600.000 Accuracy :0.899 Loss : 0.248\n",
      "epoch : 6700.000 Accuracy :0.900 Loss : 0.247\n",
      "epoch : 6800.000 Accuracy :0.901 Loss : 0.247\n",
      "epoch : 6900.000 Accuracy :0.901 Loss : 0.246\n",
      "epoch : 7000.000 Accuracy :0.901 Loss : 0.246\n",
      "epoch : 7100.000 Accuracy :0.900 Loss : 0.245\n",
      "epoch : 7200.000 Accuracy :0.901 Loss : 0.245\n",
      "epoch : 7300.000 Accuracy :0.902 Loss : 0.244\n",
      "epoch : 7400.000 Accuracy :0.900 Loss : 0.244\n",
      "epoch : 7500.000 Accuracy :0.902 Loss : 0.243\n",
      "epoch : 7600.000 Accuracy :0.901 Loss : 0.243\n",
      "epoch : 7700.000 Accuracy :0.901 Loss : 0.242\n",
      "epoch : 7800.000 Accuracy :0.901 Loss : 0.241\n",
      "epoch : 7900.000 Accuracy :0.901 Loss : 0.241\n",
      "epoch : 8000.000 Accuracy :0.901 Loss : 0.241\n",
      "epoch : 8100.000 Accuracy :0.902 Loss : 0.240\n",
      "epoch : 8200.000 Accuracy :0.901 Loss : 0.240\n",
      "epoch : 8300.000 Accuracy :0.901 Loss : 0.239\n",
      "epoch : 8400.000 Accuracy :0.901 Loss : 0.239\n",
      "epoch : 8500.000 Accuracy :0.902 Loss : 0.239\n",
      "epoch : 8600.000 Accuracy :0.902 Loss : 0.238\n",
      "epoch : 8700.000 Accuracy :0.902 Loss : 0.238\n",
      "epoch : 8800.000 Accuracy :0.903 Loss : 0.238\n",
      "epoch : 8900.000 Accuracy :0.902 Loss : 0.237\n",
      "epoch : 9000.000 Accuracy :0.902 Loss : 0.237\n",
      "epoch : 9100.000 Accuracy :0.902 Loss : 0.237\n",
      "epoch : 9200.000 Accuracy :0.902 Loss : 0.236\n",
      "epoch : 9300.000 Accuracy :0.901 Loss : 0.236\n",
      "epoch : 9400.000 Accuracy :0.902 Loss : 0.236\n",
      "epoch : 9500.000 Accuracy :0.902 Loss : 0.235\n",
      "epoch : 9600.000 Accuracy :0.901 Loss : 0.235\n",
      "epoch : 9700.000 Accuracy :0.902 Loss : 0.235\n",
      "epoch : 9800.000 Accuracy :0.902 Loss : 0.234\n",
      "epoch : 9900.000 Accuracy :0.902 Loss : 0.234\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "x,y = spiral_data(classes=3,samples=300)\n",
    "\n",
    "layer1 = Dense_layer(2,64)\n",
    "layer2 = Dense_layer(64,3)\n",
    "activation1 = Relu()\n",
    "activation2 = Softmax_and_Loss_CE()\n",
    "optimizer = Adam_Optimizer(lr=0.01, decay=1e-3)\n",
    "\n",
    "for epoch in range (10000):\n",
    "    layer1.forward(x)\n",
    "    activation1.forward(layer1.output)\n",
    "    layer2.forward(activation1.output)\n",
    "    loss = activation2.forward(layer2.output,y)\n",
    "    probs = activation2.output\n",
    "    y_true = y\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y_true = np.argmax(y,axis=1)\n",
    "    prediction = np.argmax(probs,axis=1)\n",
    "    accuracy = np.mean(y_true == prediction)\n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "        print(f'epoch : {epoch:.3f} Accuracy :{accuracy:.3f} Loss : {loss:.3f}')\n",
    "        \n",
    "    activation2.backward(probs,y)\n",
    "    layer2.backward(activation2.dinput)\n",
    "    activation1.backward(layer2.dinput)\n",
    "    layer1.backward(activation1.dinputs)\n",
    "     \n",
    "    optimizer.preupdate()\n",
    "    optimizer.update(layer1)\n",
    "    optimizer.update(layer2)\n",
    "    optimizer.postupdate()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
