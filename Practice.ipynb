{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e703b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33331734 0.3333183  0.33336434]\n",
      " [0.3332888  0.33329153 0.33341965]\n",
      " [0.33325943 0.33326396 0.33347666]\n",
      " [0.33323312 0.33323926 0.33352762]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "class dense_layer:\n",
    "    def __init__(self,n_input,neuron):\n",
    "        self.weight = 0.1*np.random.randn(n_input,neuron)\n",
    "        self.biases = np.zeros((1,neuron))\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output = np.dot(input,self.weight)+self.biases\n",
    "        return output\n",
    "    \n",
    "class Relu:\n",
    "    def forward(self,input):\n",
    "            return np.maximum(0,input)\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self,input):\n",
    "        exp = np.exp(input - np.max(input, axis=1,keepdims=True))\n",
    "        output = exp/np.sum(exp,axis=1,keepdims=True) \n",
    "        return output\n",
    "\n",
    "x,y = spiral_data(classes=3,samples=100)\n",
    "\n",
    "first_layer = dense_layer(2,3)\n",
    "output1 = first_layer.forward(x)\n",
    "\n",
    "activation = Relu()\n",
    "activation_result = activation.forward(output1)\n",
    "\n",
    "second_layer = dense_layer(3,3)\n",
    "output2 = second_layer.forward(activation_result)\n",
    "\n",
    "SM = Softmax()\n",
    "final_result = SM.forward(output2)\n",
    "\n",
    "print(final_result[:5])\n",
    "len(final_result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CROSS_ENTROPY:\n",
    "    def forward(self,target,input):\n",
    "        samples = len(input)\n",
    "        clipping = np.clip(input,1e-7,1 - 1e-7)\n",
    "        if len(input.shape) == 1:\n",
    "            confidence = clipping[range(samples),target]\n",
    "            \n",
    "        else:\n",
    "            confidence_matrix = clipping * target\n",
    "            confidence = np.sum(confidence_matrix,keepdims=True,axis=1)\n",
    "        \n",
    "        loss = -np.log(confidence)\n",
    "        \n",
    "        return loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7161d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class dense_layer:\n",
    "    def __init__(self,n_input,neurons):\n",
    "        self.weights = 0.01*np.random.randn(n_input,neurons)\n",
    "        self.bias = np.zeros((1,neurons))\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(self.inputs,self.weights)+self.bias\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "       \n",
    "class Relu :\n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        self.output = np.maximum(0,input)\n",
    "        \n",
    "    def backward(self,dvalue):\n",
    "        self.dinput = dvalue.copy()\n",
    "        self.dinput[self.input<=0] = 0\n",
    "        \n",
    "        \n",
    "class Softmax:\n",
    "    def forward(self,input):\n",
    "        exp = np.exp(input - np.max(input,axis=1,keepdims=True))\n",
    "        prob = exp/np.sum(exp,axis = 1,keepdims=True)\n",
    "        return prob\n",
    "    \n",
    "class Loss:\n",
    "    def calculate(self,y_pred,y_true):\n",
    "        loss = self.forward(y_pred,y_true)\n",
    "        total_loss = np.mean(loss)\n",
    "        return total_loss\n",
    "\n",
    "class Categorical_CE(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        \n",
    "        y_clip = np.clip(y_pred,1e-7,1-1e-7)\n",
    "        if len(y_true.shape) == 2:\n",
    "            correct_confidence = np.sum(y_clip * y_true,axis = 1)\n",
    "            \n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_clip[range(len(y_pred)),y_true ]\n",
    "            \n",
    "        negative_likhelihood = -np.log(correct_confidence)\n",
    "        \n",
    "        return negative_likhelihood\n",
    "    \n",
    "class Softmax_and_Loss_activation:\n",
    "    def __init__(self):\n",
    "        self.softmax = Softmax()\n",
    "        self.loss = Categorical_CE()\n",
    "        \n",
    "    def forward(self,input,y):\n",
    "        self.y_true = y\n",
    "        activation = self.softmax.forward(input)\n",
    "        self.output = activation\n",
    "        return self.loss.calculate(self.output,self.y_true)\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        samples = len(dvalues)\n",
    "        if len(self.y_true.shape)==2:\n",
    "            self.y_true = np.argmax(self.y_true,axis = 1)\n",
    "        self.dinput = dvalues.copy()\n",
    "        self.dinput[range(samples),self.y_true]-=1\n",
    "        self.dinput = self.dinput/samples\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    def __init__(self,learning_rate=1.,decay=0.,momentum = 0,epsilon=1e-7):\n",
    "        self.lr = learning_rate\n",
    "        self.current_learning = learning_rate\n",
    "        self.decay = decay\n",
    "        self.itteration = 0\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def preupdate(self):\n",
    "        if self.decay:\n",
    "            self.current_learning = self.lr / (1 + (self.decay * self.itteration))\n",
    "    def update(self,layer):\n",
    "        # if self.momentum :\n",
    "\n",
    "        #     if not hasattr(layer, 'weight_momentums'):\n",
    "        #         layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "        #         layer.bias_momentums = np.zeros_like(layer.bias)\n",
    "                \n",
    "        #     weight_update = -(self.current_learning *layer.dweights )+(layer.weight_momentums * self.momentum) #direction\n",
    "        #     layer.weight_momentums = weight_update \n",
    "        #     bias_update = -(self.current_learning *layer.dbiases )+(layer.bias_momentums * self.momentum)\n",
    "        #     layer.bias_momentums = bias_update  \n",
    "            \n",
    "        # else:    \n",
    "        #     weight_update = -self.current_learning *layer.dweights\n",
    "        #     bias_update   = -self.current_learning * layer.dbiases\n",
    "            \n",
    "        # layer.weights += weight_update\n",
    "        # layer.bias += bias_update\n",
    "            if not hasattr(layer,'weight_cache'):\n",
    "                layer.weight_cache = np.zeros_like(layer.weights)\n",
    "                layer.bias_cache = np.zeros_like(layer.bias)\n",
    "                \n",
    "            layer.weight_cache += layer.dweights**2\n",
    "            layer.bias_cache += layer.dbiases**2\n",
    "            \n",
    "            layer.weights += -self.current_learning * layer.dweights / (np.sqrt(layer.weight_cache + self.epsilon))\n",
    "            layer.bias += -self.current_learning * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)    \n",
    "        \n",
    "        \n",
    "    def postupdate(self):\n",
    "        self.itteration+=1\n",
    "\n",
    "        \n",
    "class Optimizer_ADAGRAD:\n",
    "    def __init__(self,layer,epsilon=1e-7,decay=0.,lr=1):\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.current_lr = lr\n",
    "        self.itteration = 0\n",
    "    def preupdate(self):\n",
    "        self.current_lr = self.lr / (1 + (self.decay * self.itteration))\n",
    "    \n",
    "    def update(self,layer):\n",
    "        if not hasattr(layer,'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.bias)\n",
    "                \n",
    "            layer.weight_cache += layer.dweights**2\n",
    "            layer.bias_cache += layer.dbiases**2\n",
    "            \n",
    "            layer.weights += -self.current_learning * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "            layer.bias += -self.current_learning * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)    \n",
    "        \n",
    "        \n",
    "    def postupdate(self):\n",
    "        self.itteration+=1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c42ee3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.333 loss : 1.099\n",
      "epoch: 100, acc: 0.418 loss : 1.083\n",
      "epoch: 200, acc: 0.418 loss : 1.072\n",
      "epoch: 300, acc: 0.423 loss : 1.071\n",
      "epoch: 400, acc: 0.421 loss : 1.071\n",
      "epoch: 500, acc: 0.419 loss : 1.070\n",
      "epoch: 600, acc: 0.417 loss : 1.070\n",
      "epoch: 700, acc: 0.420 loss : 1.070\n",
      "epoch: 800, acc: 0.420 loss : 1.070\n",
      "epoch: 900, acc: 0.422 loss : 1.069\n",
      "epoch: 1000, acc: 0.427 loss : 1.068\n",
      "epoch: 1100, acc: 0.428 loss : 1.067\n",
      "epoch: 1200, acc: 0.429 loss : 1.066\n",
      "epoch: 1300, acc: 0.432 loss : 1.064\n",
      "epoch: 1400, acc: 0.432 loss : 1.061\n",
      "epoch: 1500, acc: 0.433 loss : 1.058\n",
      "epoch: 1600, acc: 0.436 loss : 1.055\n",
      "epoch: 1700, acc: 0.436 loss : 1.050\n",
      "epoch: 1800, acc: 0.438 loss : 1.046\n",
      "epoch: 1900, acc: 0.434 loss : 1.040\n",
      "epoch: 2000, acc: 0.453 loss : 1.035\n",
      "epoch: 2100, acc: 0.470 loss : 1.029\n",
      "epoch: 2200, acc: 0.467 loss : 1.023\n",
      "epoch: 2300, acc: 0.480 loss : 1.016\n",
      "epoch: 2400, acc: 0.474 loss : 1.010\n",
      "epoch: 2500, acc: 0.459 loss : 1.003\n",
      "epoch: 2600, acc: 0.462 loss : 0.997\n",
      "epoch: 2700, acc: 0.468 loss : 0.992\n",
      "epoch: 2800, acc: 0.477 loss : 0.986\n",
      "epoch: 2900, acc: 0.483 loss : 0.981\n",
      "epoch: 3000, acc: 0.492 loss : 0.974\n",
      "epoch: 3100, acc: 0.498 loss : 0.967\n",
      "epoch: 3200, acc: 0.506 loss : 0.960\n",
      "epoch: 3300, acc: 0.514 loss : 0.954\n",
      "epoch: 3400, acc: 0.521 loss : 0.948\n",
      "epoch: 3500, acc: 0.522 loss : 0.942\n",
      "epoch: 3600, acc: 0.520 loss : 0.937\n",
      "epoch: 3700, acc: 0.524 loss : 0.932\n",
      "epoch: 3800, acc: 0.533 loss : 0.927\n",
      "epoch: 3900, acc: 0.541 loss : 0.922\n",
      "epoch: 4000, acc: 0.550 loss : 0.917\n",
      "epoch: 4100, acc: 0.560 loss : 0.912\n",
      "epoch: 4200, acc: 0.569 loss : 0.907\n",
      "epoch: 4300, acc: 0.574 loss : 0.902\n",
      "epoch: 4400, acc: 0.582 loss : 0.897\n",
      "epoch: 4500, acc: 0.592 loss : 0.892\n",
      "epoch: 4600, acc: 0.602 loss : 0.887\n",
      "epoch: 4700, acc: 0.600 loss : 0.883\n",
      "epoch: 4800, acc: 0.599 loss : 0.878\n",
      "epoch: 4900, acc: 0.599 loss : 0.873\n",
      "epoch: 5000, acc: 0.601 loss : 0.869\n",
      "epoch: 5100, acc: 0.599 loss : 0.864\n",
      "epoch: 5200, acc: 0.590 loss : 0.860\n",
      "epoch: 5300, acc: 0.593 loss : 0.855\n",
      "epoch: 5400, acc: 0.598 loss : 0.851\n",
      "epoch: 5500, acc: 0.602 loss : 0.847\n",
      "epoch: 5600, acc: 0.609 loss : 0.842\n",
      "epoch: 5700, acc: 0.616 loss : 0.838\n",
      "epoch: 5800, acc: 0.619 loss : 0.834\n",
      "epoch: 5900, acc: 0.620 loss : 0.830\n",
      "epoch: 6000, acc: 0.620 loss : 0.826\n",
      "epoch: 6100, acc: 0.622 loss : 0.823\n",
      "epoch: 6200, acc: 0.621 loss : 0.819\n",
      "epoch: 6300, acc: 0.623 loss : 0.815\n",
      "epoch: 6400, acc: 0.627 loss : 0.812\n",
      "epoch: 6500, acc: 0.630 loss : 0.808\n",
      "epoch: 6600, acc: 0.633 loss : 0.805\n",
      "epoch: 6700, acc: 0.636 loss : 0.802\n",
      "epoch: 6800, acc: 0.631 loss : 0.798\n",
      "epoch: 6900, acc: 0.630 loss : 0.795\n",
      "epoch: 7000, acc: 0.630 loss : 0.792\n",
      "epoch: 7100, acc: 0.629 loss : 0.789\n",
      "epoch: 7200, acc: 0.629 loss : 0.786\n",
      "epoch: 7300, acc: 0.631 loss : 0.783\n",
      "epoch: 7400, acc: 0.628 loss : 0.781\n",
      "epoch: 7500, acc: 0.628 loss : 0.778\n",
      "epoch: 7600, acc: 0.627 loss : 0.776\n",
      "epoch: 7700, acc: 0.627 loss : 0.773\n",
      "epoch: 7800, acc: 0.627 loss : 0.770\n",
      "epoch: 7900, acc: 0.627 loss : 0.768\n",
      "epoch: 8000, acc: 0.629 loss : 0.766\n",
      "epoch: 8100, acc: 0.627 loss : 0.763\n",
      "epoch: 8200, acc: 0.623 loss : 0.761\n",
      "epoch: 8300, acc: 0.627 loss : 0.759\n",
      "epoch: 8400, acc: 0.630 loss : 0.757\n",
      "epoch: 8500, acc: 0.631 loss : 0.754\n",
      "epoch: 8600, acc: 0.633 loss : 0.753\n",
      "epoch: 8700, acc: 0.637 loss : 0.751\n",
      "epoch: 8800, acc: 0.636 loss : 0.749\n",
      "epoch: 8900, acc: 0.637 loss : 0.748\n",
      "epoch: 9000, acc: 0.637 loss : 0.746\n",
      "epoch: 9100, acc: 0.640 loss : 0.744\n",
      "epoch: 9200, acc: 0.640 loss : 0.742\n",
      "epoch: 9300, acc: 0.641 loss : 0.740\n",
      "epoch: 9400, acc: 0.641 loss : 0.736\n",
      "epoch: 9500, acc: 0.641 loss : 0.733\n",
      "epoch: 9600, acc: 0.654 loss : 0.730\n",
      "epoch: 9700, acc: 0.660 loss : 0.727\n",
      "epoch: 9800, acc: 0.664 loss : 0.724\n",
      "epoch: 9900, acc: 0.662 loss : 0.722\n",
      "epoch: 10000, acc: 0.668 loss : 0.719\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "x,y = spiral_data(classes=3,samples=300)\n",
    "\n",
    "layer1 = dense_layer(2,64)\n",
    "activation1 = Relu()\n",
    "layer2 = dense_layer(64,3)\n",
    "Softmaxandloss = Softmax_and_Loss_activation()\n",
    "optimizer = Optimizer_SGD(decay=1e-3)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    layer1.forward(x)\n",
    "\n",
    "    activation1.forward(layer1.output)\n",
    "\n",
    "    layer2.forward(activation1.output)\n",
    "\n",
    "    loss = Softmaxandloss.forward(layer2.output,y)\n",
    "\n",
    "    prediction = np.argmax(Softmaxandloss.output,axis=1)\n",
    "    if len(y.shape)==2:\n",
    "        y = np.argmax(y,axis = 1)\n",
    "    accuracy = np.mean(prediction==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, '+\n",
    "            f'acc: {accuracy:.3f} '+\n",
    "            f'loss : {loss:.3f}')\n",
    "\n",
    "\n",
    "    Softmaxandloss.backward(Softmaxandloss.output)\n",
    "    layer2.backward(Softmaxandloss.dinput)\n",
    "    activation1.backward(layer2.dinputs)\n",
    "    layer1.backward(activation1.dinput)\n",
    "    \n",
    "    optimizer.preupdate()\n",
    "    optimizer.update(layer1)\n",
    "    optimizer.update(layer2)\n",
    "    optimizer.postupdate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
